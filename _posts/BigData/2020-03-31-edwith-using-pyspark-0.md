---
date: 2020-03-31 08:24:19
layout: post
title: "[PySpark를 활용한 데이터분석] 0. 강의 설명"
subtitle:
description:
image: https://user-images.githubusercontent.com/31606119/78007699-979b5d00-7379-11ea-9a2a-934e782d59c9.jpeg
optimized_image: https://user-images.githubusercontent.com/31606119/78007699-979b5d00-7379-11ea-9a2a-934e782d59c9.jpeg
category: BigData
tags:
  - BigData
  - Spark
author: JHLeeeMe
paginate: false
---

![PySpark_대표이미지](https://user-images.githubusercontent.com/31606119/78007699-979b5d00-7379-11ea-9a2a-934e782d59c9.jpeg)

한달 전 PySpark을 공부할 때 edwith에서 들었던 강의를 정리해서 올리려고 한다.
### 강의 링크: [PySpark를 활용한 데이터분석](https://www.edwith.org/sparktutorial)  
### Spark Doc 한글문서  
[https://spark-korea.github.io/docs/](https://spark-korea.github.io/docs/)

---

### PySpark를 활용한 탐색적 데이터 분석(EDA) 강의

### 강의는 챕터가 총 5개로
-- Chapter 0. 강의에 대한 설명 및 추천 학습 방법  
-- Chapter 1. 환경 설정하기  
-- Chapter 2. 데이터 분석을 위한 준비 작업  
-- Chapter 3. 예제로 배우는 Spark  
-- Chapter 4. 이후의 학습 방법  

이렇게 있다.  
강의에서의 Spark 환경은 Databricks Cloud를 이용하는 방법인데,  
본인은 로컬환경에 Zeppelin으로 학습을 진행 하므로 ```Chapter 1``` 은 aws Access Key 부분 말고는 넘어갈 예정이다.  
학습 데이터를 aws S3에서 가져와야 하기 때문이다.

로컬 환경에서 Spark, Zeppelin 환경 구성하는 방법은 본인 블로그에 기재 해놨다.  
[Spark & Zeppelin 설치하기](https://jhleeeme.github.io/how-to-install-spark-and-zeppelin/)

